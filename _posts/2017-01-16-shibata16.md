---
title: Predicting Sequential Data with LSTMs Augmented with Strictly 2-Piecewise Input
  Vectors
abstract: Recurrent neural networks such as Long-Short Term Memory (LSTM) are often
  used to learn from various kinds of time-series data, especially those that involved
  long-distance dependencies.  We introduce a vector representation for the Strictly
  2-Piecewise (SP-2) formal languages, which encode certain kinds of long-distance
  dependencies using subsequences. These vectors are added to the LSTM architecture
  as an additional input. Through experiments with the problems in the SPiCe dataset,
  we demonstrate that for certain problems, these vectors slightly—but significantly—improve
  the top-5 score (normalized discounted cumulative gain) as well as the accuracy
  as compared to the LSTM architecture without the SP-2 input vector. These results
  are also compared to an LSTM architecture with an input vector based on bigrams.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shibata16
month: 0
tex_title: Predicting Sequential Data with {LSTM}s Augmented with Strictly 2-Piecewise
  Input Vectors
firstpage: 137
lastpage: 142
page: 137-142
order: 137
cycles: false
author:
- given: Chihiro
  family: Shibata
- given: Jeffrey
  family: Heinz
date: 2017-01-16
address: Delft, The Netherlands
publisher: PMLR
container-title: Proceedings of The 13th International Conference on Grammatical Inference
volume: '57'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 1
  - 16
pdf: http://proceedings.mlr.press/v57/shibata16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
