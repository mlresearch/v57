<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Predicting Sequential Data with <span>LSTM</span>s Augmented with Strictly 2-Piecewise Input Vectors | ICGI 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Predicting Sequential Data with {LSTM}s Augmented with Strictly 2-Piecewise Input Vectors">

  <meta name="citation_author" content="Shibata, Chihiro">

  <meta name="citation_author" content="Heinz, Jeffrey">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of The 13th International Conference on Grammatical Inference">
<meta name="citation_firstpage" content="137">
<meta name="citation_lastpage" content="142">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v57/shibata16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Predicting Sequential Data with <span>LSTM</span>s Augmented with Strictly 2-Piecewise Input Vectors</h1>

	<div id="authors">
	
		Chihiro Shibata,
	
		Jeffrey Heinz
	<br />
	</div>
	<div id="info">
		Proceedings of The 13th International Conference on Grammatical Inference,
		pp. 137–142, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Recurrent neural networks such as Long-Short Term Memory (LSTM) are often used to learn from various kinds of time-series data, especially those that involved long-distance dependencies. We introduce a vector representation for the Strictly 2-Piecewise (SP-<span class="math">\(2\)</span>) formal languages, which encode certain kinds of long-distance dependencies using subsequences. These vectors are added to the LSTM architecture as an additional input. Through experiments with the problems in the SPiCe dataset, we demonstrate that for certain problems, these vectors slightly—but significantly—improve the top-5 score (normalized discounted cumulative gain) as well as the accuracy as compared to the LSTM architecture without the SP-<span class="math">\(2\)</span> input vector. These results are also compared to an LSTM architecture with an input vector based on bigrams.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="shibata16.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
